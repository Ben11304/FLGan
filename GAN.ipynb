{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_work import processed\n",
    "df=pd.read_csv(\"/Users/mac/Dev/data/dta_IoT/new2test.csv\")\n",
    "#delete all null columns\n",
    "id=[16,17,21,22,23,24]\n",
    "col=df.columns\n",
    "for idx in id:\n",
    "    df=df.drop(col[idx],axis=1)\n",
    "data=processed(df,\"subcategory \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>pkSeqID</th>\n",
       "      <th>stime</th>\n",
       "      <th>flgs</th>\n",
       "      <th>proto</th>\n",
       "      <th>saddr</th>\n",
       "      <th>sport</th>\n",
       "      <th>daddr</th>\n",
       "      <th>dport</th>\n",
       "      <th>pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>max</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>srate</th>\n",
       "      <th>drate</th>\n",
       "      <th>attack</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1982</td>\n",
       "      <td>2490</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4838</td>\n",
       "      <td>16</td>\n",
       "      <td>528</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5114</td>\n",
       "      <td>2623</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4669</td>\n",
       "      <td>24</td>\n",
       "      <td>901</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>908</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6841</td>\n",
       "      <td>4276</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2039</td>\n",
       "      <td>25</td>\n",
       "      <td>901</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2132</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>6835</td>\n",
       "      <td>4272</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>25</td>\n",
       "      <td>901</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2131</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>555</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1216</td>\n",
       "      <td>1126</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2134</td>\n",
       "      <td>27</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>577</td>\n",
       "      <td>762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021</th>\n",
       "      <td>2</td>\n",
       "      <td>2995</td>\n",
       "      <td>5569</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1062</td>\n",
       "      <td>20</td>\n",
       "      <td>372</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>2443</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>462</td>\n",
       "      <td>130</td>\n",
       "      <td>1574</td>\n",
       "      <td>1267</td>\n",
       "      <td>1037</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>4</td>\n",
       "      <td>2611</td>\n",
       "      <td>2271</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3079</td>\n",
       "      <td>20</td>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>7</td>\n",
       "      <td>6744</td>\n",
       "      <td>4190</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1293</td>\n",
       "      <td>25</td>\n",
       "      <td>901</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2165</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>372</td>\n",
       "      <td>556</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>1</td>\n",
       "      <td>4610</td>\n",
       "      <td>4966</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3931</td>\n",
       "      <td>20</td>\n",
       "      <td>900</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1940</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>81</td>\n",
       "      <td>586</td>\n",
       "      <td>398</td>\n",
       "      <td>761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>0</td>\n",
       "      <td>3971</td>\n",
       "      <td>5438</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1979</td>\n",
       "      <td>20</td>\n",
       "      <td>372</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>1957</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>462</td>\n",
       "      <td>130</td>\n",
       "      <td>1714</td>\n",
       "      <td>1374</td>\n",
       "      <td>1149</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7026 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subcategory   pkSeqID  stime  flgs  proto  saddr  sport  daddr  dport  \\\n",
       "0                4     1982   2490     0      2      5   4838     16    528   \n",
       "1                6     5114   2623     7      2      3   4669     24    901   \n",
       "2                7     6841   4276     0      3      0   2039     25    901   \n",
       "3                7     6835   4272     0      3      0   2002     25    901   \n",
       "4                3     1216   1126     0      3      1   2134     27    697   \n",
       "...            ...      ...    ...   ...    ...    ...    ...    ...    ...   \n",
       "7021             2     2995   5569     0      2      3   1062     20    372   \n",
       "7022             4     2611   2271     0      2      3   3079     20    872   \n",
       "7023             7     6744   4190     0      3      0   1293     25    901   \n",
       "7024             1     4610   4966     0      2      2   3931     20    900   \n",
       "7025             0     3971   5438     0      2      3   1979     20    372   \n",
       "\n",
       "      pkts  ...   max  spkts  dpkts  sbytes  dbytes  rate  srate  drate  \\\n",
       "0        1  ...    29      1      0      30       0     0      0      0   \n",
       "1        4  ...   908      3      1      74       1    35     22      0   \n",
       "2        9  ...  2132      9      0      71       0   370    554      0   \n",
       "3        9  ...  2131      9      0      71       0   371    555      0   \n",
       "4        1  ...     0      1      0      41       0   577    762      0   \n",
       "...    ...  ...   ...    ...    ...     ...     ...   ...    ...    ...   \n",
       "7021    27  ...  2443     14     15     462     130  1574   1267   1037   \n",
       "7022     1  ...    73      0      1       0       1  2417      0      0   \n",
       "7023     9  ...  2165      9      0      71       0   372    556      0   \n",
       "7024     9  ...  1940      5      4     240      81   586    398    761   \n",
       "7025    27  ...  1957     14     15     462     130  1714   1374   1149   \n",
       "\n",
       "      attack  category  \n",
       "0          1         2  \n",
       "1          1         0  \n",
       "2          1         0  \n",
       "3          1         0  \n",
       "4          0         1  \n",
       "...      ...       ...  \n",
       "7021       1         3  \n",
       "7022       1         2  \n",
       "7023       1         0  \n",
       "7024       1         0  \n",
       "7025       1         3  \n",
       "\n",
       "[7026 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data=torch.tensor(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   4, 1982, 2490,  ...,    0,    1,    2],\n",
       "        [   6, 5114, 2623,  ...,    0,    1,    0],\n",
       "        [   7, 6841, 4276,  ...,    0,    1,    0],\n",
       "        ...,\n",
       "        [   7, 6744, 4190,  ...,    0,    1,    0],\n",
       "        [   1, 4610, 4966,  ...,  761,    1,    0],\n",
       "        [   0, 3971, 5438,  ..., 1149,    1,    3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7026, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        label = labels[i]\n",
    "        encoded_labels[i, label] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(generator, self).__init__()\n",
    "        self.input_dim = args.noise_size\n",
    "        self.output_dim = args.n_features\n",
    "        self.class_num = args.n_classes\n",
    "        self.label_emb = nn.Embedding(self.class_num,self.class_num)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "    \n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.input_dim + self.class_num, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024,self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise ,label):\n",
    "        x = torch.cat((self.label_emb(label).squeeze(),noise), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.input_dim = args.n_features\n",
    "        self.output_dim = args.n_features\n",
    "        self.class_num = args.n_classes\n",
    "        self.label_emb = nn.Embedding(self.class_num,self.class_num)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear((self.class_num + self.input_dim), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(256, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(256, self.class_num), nn.Softmax())\n",
    "        \n",
    "\n",
    "    def forward(self, input ,label):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        x = torch.cat((self.label_emb(label).squeeze(), input), 1)\n",
    "        x=self.model(x)\n",
    "        real=self.adv_layer(x)\n",
    "        label=self.aux_layer(x)\n",
    "        return real,label \n",
    "    \n",
    "class CGAN(object):\n",
    "    def __init__(self, args):\n",
    "        # parameters\n",
    "        self.epoch = args.epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.save_dir = args.save_dir\n",
    "        self.result_dir = args.result_dir\n",
    "        self.dataset = args.dataset\n",
    "        self.log_dir = args.log_dir\n",
    "        self.gpu_mode = args.gpu_mode\n",
    "        self.model_name = args.gan_type\n",
    "        self.z_dim = args.z_dim\n",
    "        self.class_num = args.n_class\n",
    "        # load dataset\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        data=next(iter(self.data_loader))\n",
    "        #option\n",
    "        dim=int(data.shape[1])-8\n",
    "\n",
    "        # networks init\n",
    "        self.G = generator(args)\n",
    "        self.D = discriminator(args)\n",
    "        self.G_optimizer = optim.RMSprop(self.G.parameters(), lr=args.lrG, alpha=0.9)\n",
    "        self.D_optimizer = optim.RMSprop(self.D.parameters(), lr=args.lrD, alpha=0.9)\n",
    "        \n",
    "        self.MSE_loss = torch.nn.MSELoss()\n",
    "        self.BCE_loss=nn.BCELoss()\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = torch.nn.BCELoss()\n",
    "        self.auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "        print('---------- Networks architecture -------------')\n",
    "        utils.print_network(self.G)\n",
    "        utils.print_network(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "        self.y_real_ = Variable(torch.FloatTensor(self.batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        self.y_fake_ = Variable(torch.FloatTensor(self.batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        #self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1), torch.zeros(self.batch_size, 1)\n",
    "\n",
    "\n",
    "\n",
    "        self.D.train()\n",
    "        print('training start!!')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, da in enumerate(self.data_loader):\n",
    "                x_=da[:,1:].float()\n",
    "                y_=da[:,:1].int()\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = Variable(torch.FloatTensor(np.random.normal(0, 1, (self.batch_size, self.z_dim))))\n",
    "                y_check= onehot_encode(y_,8)\n",
    "                y_check=torch.tensor(y_check)\n",
    "                # update D network\n",
    "                self.D.train()\n",
    "                self.G.eval()\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                real,label = self.D(x_, y_)\n",
    "                #D_real_loss = self.MSE_loss(D_real, self.y_real_)\n",
    "                G_ = self.G(z_, y_)\n",
    "                fake,label_ = self.D(G_, y_)\n",
    "                #D_fake_loss = self.MSE_loss(D_fake, self.y_fake_)\n",
    "                D_fake_loss=(self.adversarial_loss(fake, self.y_fake_) + self.auxiliary_loss(label_, y_check)) / 2\n",
    "                D_real_loss=(self.adversarial_loss(real, self.y_real_) + self.auxiliary_loss(label, y_check)) / 2\n",
    "\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "                self.train_hist['D_loss'].append(D_loss.item())\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                 # update G network\n",
    "                self.D.eval()\n",
    "                self.G.train()\n",
    "                self.G_optimizer.zero_grad()\n",
    "\n",
    "                z_=torch.rand((self.batch_size, self.z_dim))\n",
    "                G_ = self.G(z_, y_)\n",
    "                real,label = self.D(G_, y_)\n",
    "                #G_loss = self.MSE_loss(D_fake, self.y_real_)\n",
    "\n",
    "                G_loss=0.5 * (self.adversarial_loss(real, self.y_real_) + self.auxiliary_loss(label, y_check))\n",
    "                self.train_hist['G_loss'].append(G_loss.item())\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "               \n",
    "                if (iter + 1) == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "                \n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              self.epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self,data):\n",
    "        self.epoch = 10\n",
    "        self.batch_size = 32\n",
    "        self.dataset = data\n",
    "        self.log_dir = 'logs/'\n",
    "        self.gpu_mode = True\n",
    "        self.gan_type = 'cGAN'\n",
    "        self.z_dim = 10\n",
    "        self.n_class = 8\n",
    "        self.sample_num = self.n_class ** 2\n",
    "        self.lrG=0.0001\n",
    "        self.lrD=0.0001\n",
    "        self.n_epochs=self.n_class\n",
    "        self.n_classes=self.n_class\n",
    "        self.noise_size=self.z_dim\n",
    "        self.n_features=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   4, 1982, 2490,  ...,    0,    1,    2],\n",
      "        [   6, 5114, 2623,  ...,    0,    1,    0],\n",
      "        [   7, 6841, 4276,  ...,    0,    1,    0],\n",
      "        ...,\n",
      "        [   7, 6744, 4190,  ...,    0,    1,    0],\n",
      "        [   1, 4610, 4966,  ...,  761,    1,    0],\n",
      "        [   0, 3971, 5438,  ..., 1149,    1,    3]])\n"
     ]
    }
   ],
   "source": [
    "arg=Args(data)\n",
    "print(arg.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks architecture -------------\n",
      "generator(\n",
      "  (label_emb): Embedding(8, 8)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Linear(in_features=1024, out_features=28, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 724700\n",
      "discriminator(\n",
      "  (label_emb): Embedding(8, 8)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=36, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (6): Dropout(p=0.4, inplace=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (aux_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=8, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 677961\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=CGAN(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1] [ 219/ 219] D_loss: 2.17477554, G_loss: 1.36390997\n",
      "Epoch: [ 2] [ 219/ 219] D_loss: 2.14600681, G_loss: 1.33152789\n",
      "Epoch: [ 3] [ 219/ 219] D_loss: 2.42999448, G_loss: 1.55344171\n",
      "Epoch: [ 4] [ 219/ 219] D_loss: 1.87071966, G_loss: 1.38079383\n",
      "Epoch: [ 5] [ 219/ 219] D_loss: 1.70050527, G_loss: 1.48585143\n",
      "Epoch: [ 6] [ 219/ 219] D_loss: 1.66830478, G_loss: 1.42419575\n",
      "Epoch: [ 7] [ 219/ 219] D_loss: 1.61615343, G_loss: 1.52105531\n",
      "Epoch: [ 8] [ 219/ 219] D_loss: 1.62579116, G_loss: 1.03923865\n",
      "Epoch: [ 9] [ 219/ 219] D_loss: 1.54864334, G_loss: 1.83200431\n",
      "Epoch: [10] [ 219/ 219] D_loss: 1.44122551, G_loss: 1.87926778\n",
      "Avg one epoch time: 2.59, total 10 epochs time: 25.87\n",
      "Training finish!\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.tensor([[1],[2],[3],[4],[5],[6],[7],[0],[1],[2],[3],[4],[5],[6],[7],[0],[1],[2],[3],[4],[5],[6],[7],[0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len=y.shape[0]\n",
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len=y.shape[0]\n",
    "z= torch.rand(len,10)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=torch.cat([z,y],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake=model.G(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
